{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import OperationFailure\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conexión a Mongo Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb+srv://vitoesmoris:1234@cluster0.kdeu0.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "\n",
    "# Variables de la BD\n",
    "database = 'data_rag'\n",
    "collection = client[\"data_rag\"][\"rag_collection\"]\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME='index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = ['naturaleza-yo.pdf']\n",
    "all_documents = []\n",
    "\n",
    "for document in pdf_files:\n",
    "    loader = PyPDFLoader(document) # Carga el documento\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)    # Divide el texto en chunks\n",
    "    documents = text_splitter.split_documents(data)\n",
    "    all_documents.extend(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado de datos en vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/LaBSE\")\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    relevance_score_fn=\"cosine\",\n",
    ")\n",
    "\n",
    "vector_store.add_documents(all_documents)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de índice de búsqueda vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El índice 'index' ya existe.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store.create_vector_search_index(dimensions=384)\n",
    "except OperationFailure as e:\n",
    "    if e.code == 68:\n",
    "        print(f\"El índice '{ATLAS_VECTOR_SEARCH_INDEX_NAME}' ya existe.\")\n",
    "    else:\n",
    "        print(f\"Error al crear el índice: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de lenguaje (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Responde en base al contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Local LLM: Se configura el modelo llama3.2 para generar respuestas de manera local\n",
    "ollama_llm = \"llama3.2\"\n",
    "model_local = ChatOllama(model=ollama_llm)\n",
    "\n",
    "# Chain Se construye un pipeline que pasa el contexto desde el retriever\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No tengo información específica sobre el nombre alternativo de una persona llamada Manuel. ¿Podrías proporcionar más contexto o detalles sobre la persona a la que te refieres?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Qué otro nombre tiene Manuel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Qué pasa al final de la historia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
